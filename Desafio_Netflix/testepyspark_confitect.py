# -*- coding: utf-8 -*-
"""TESTEPYSPARK_Confitect.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_cZeB0nyZwZA81oXW4NfFGLlnHVvK208

## Instalando a Biblioteca, cujas funções permitirão se conectar e manipular dados de Buckets do AWS S3
"""

!pip install boto3

"""## Importando as funções / bibliotecas necessárias para conexão, upload, tratamento de erros e  manipulação de arquivos, tanto no S3 como no file system local"""

import boto3
from datetime import datetime
import botocore
from botocore.exceptions import ProfileNotFound, ClientError
# from botocore.exceptions import ClientError
import os

"""## Conectando ao S3 e extraindo o arquivo parquet que servirá como fonte de dados para o processo

Nesta próxima célula eu instancio as variáveis de conexão com um Bucket do s3 chamado 'detch-bucket01', cuja criação se deu através do portal da AWS. 
Também crio uma pasta no file system local para armazenar o arquivo parquet, que também foi baixado e salvo no Bucket 'detch-bucket01' manualmente
"""

Buket = 'detch-bucket01'
File = "OriginaisNetflix20221230.parquet"
path = "load_data_Netflix/OriginaisNetflix20221230.parquet"
ACCESS_ID = 'AKIA6D667OAARISPB2OJ'
ACCESS_KEY =  'nTFYpTubQdmgkUsqWnwxBZSCUz4VqSEX5ySynXMK'
s3 = boto3.resource('s3',
      aws_access_key_id=ACCESS_ID,
      aws_secret_access_key= ACCESS_KEY)

today = datetime.today()
dt_string = today.strftime("%Y%m%d")
print("date =", dt_string)

File_extract = dt_string + "OriginaisNetflix.parquet"

folder_extract = 'sourceNetflix'

path_source = Buket + '/' + folder_extract + '/' + File_extract

if not os.path.exists(Buket):
    os.mkdir(Buket)
    os.mkdir(folder_extract)

# if not os.path.exists(folder_extract):
#     os.mkdir(folder_extract)
  
print(path_source)

"""## Conectando ao Buket S3 e fazendo download do arquivo de origem dos dados"""

try:
  s3.Bucket(Buket).download_file(path, path_source)
  print("successful extraction!")
  
except botocore.exceptions.ClientError as e:
  if e.response['Error']['Code'] == "404":
    print("file not found.")
  else:
    raise

"""## Instalando a biblioteca spark cujas funções permitirão processar e manipular um grande volume de dados"""

!pip install pyspark==3.3.1

"""## Importando os pacotes e funções do spark que serão úteis """

import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import DateType, TimestampType
from datetime import timedelta
from dateutil import parser

"""## Instanciando o Spark e salvando em um Dataframe o conteúdo dos dados de origem"""

spark=SparkSession.builder.appName("PySpark Read Netflix Parquet").getOrCreate()
df_Netflix = spark.read.parquet(path_source)
df_Netflix.show()

"""##Nesta célula são salvos em uma lista todos os campos com suas alterações solicitadas e essa lista será passada em um select mais adiante.

Dessa forma além de definir um esquema fixo, o uso de funções como WithColoumn (que podem se tornar bem custosas em termos de processamento) torna-se desnecessário. 
"""

#essa função 'Date_format' realiza a alteração do formato '13-Mai-2019' para o formato '2019-05-13' e o tipo date(um dos tipos de datetime)
Date_format = udf(lambda linha: parser.parse(linha), DateType())

cols = [
    col('Title').alias('Título'),
    col('Genre').alias('Classificação de Gênero'),
    Date_format(col("Premiere")).alias("Pré-Estreia"),
    regexp_replace('Seasons', 'TBA', 'a ser anunciado').alias('Temporadas'),
    col('SeasonsParsed').alias('Temporadas Disponíveis'),
    col('EpisodesParsed').alias('Episódios Disponíveis'),
    col('Length').alias('Duração Episódios'),
    col('MinLength').alias('Duração Mínima Episódios'),
    col('MaxLength').alias('Duração Máxima Episódios'),
    col('Status').alias('Situação'),
    col('Active').alias('Progressão'),
    col('Table').alias('Classificação Principal'),
    col('Language').alias('Idioma Oririginal'),
    (to_timestamp('dt_inclusao') - timedelta(hours=3)).alias('Data Inclusão'),
    (current_timestamp() - timedelta(hours=3)).alias('Data Alteração')
]

"""## Nesta próxima célula eu salvo no mesmo dataframe os campos com todas as alterações solicitadas, através de um select, e já retiro algum possível dado duplicado."""

df_Netflix = (
    df_Netflix
    .select(*cols)
    .dropDuplicates(['Título', 'Temporadas', 'Pré-Estreia'])
)

"""Faço uma contagem de linhas e uma contagem distinta para verificar se há diferença. A ideia é que após o comando de deduplicaçã esses valores sejam iguais."""

df_Netflix.distinct().count()

df_Netflix.count()

"""## Nesta próxima célula apenas os campos requisitados para o arquivo .csv são selecionados, junto à ordenação dos dados solicitadas """

df_csv = (
    df_Netflix
    .select(
        'Título',
        'Classificação de Gênero',
        'Temporadas',
        'Pré-Estreia',
        'Idioma Oririginal',
        'Situação',
        'Progressão',
        'Data Inclusão',
        'Data Alteração'
    ).orderBy(
        col('Progressão').desc(), 
        col('Classificação de Gênero').desc()
        )
)


df_csv.show()

"""## Aqui uma nova pasta no Bucket principal é criada para armazenar o csv resultante"""

Bucket_destination = 'detch-bucket01'
directory_destination = "refined_data_Netflix/" 
all_obj_bucket = list(s3.Bucket('detch-bucket01').objects.all()) # listando tudo que tem no Bucket principal

# para esse tipo de alteração usaremos o CLI do s3 e aqui setamos as credenciais necessárias
s3_client = boto3.client('s3',
      aws_access_key_id=ACCESS_ID,
      aws_secret_access_key= ACCESS_KEY)

# só faremos a criação desse novo diretório, caso o mesmo já não exista
if s3.ObjectSummary(Bucket_destination, directory_destination) not in all_obj_bucket:
  print('Diretório não existente...  criando')
  s3_client.put_object(Bucket=Bucket_destination, Key=(directory_destination + '/'))#comando que realiza a criação do novo diretório no s3
else:
  print('Diretório existente... ')

print("O diretório de destino no s3 é: ")
print(s3.ObjectSummary(Bucket_destination, directory_destination).Bucket().name + '/' + s3.ObjectSummary(Bucket_destination, directory_destination).key)

"""## Salvaremos e renomearemos o arquivo .csv no sistema local de arquivos, antes de fazer um upload para a pasta de destino do Buacket principal do S3"""

folder_destination_sytem = 'refined_data_Netflix'
os.path.exists(Bucket_destination +'/' + folder_destination_sytem)

folder_destination_sytem = 'refined_data_Netflix'

#crio a pasta local onde o .csv será armazenado, caso ela já não exista
if not os.path.exists(Bucket_destination +'/' + folder_destination_sytem):
  os.mkdir(Bucket_destination +'/' + folder_destination_sytem)

# path local onde o .csv será armazenado
path_FileCsv_system = Bucket_destination +'/' + folder_destination_sytem +'/' + dt_string + 'Table_Series_Netflix.csv'

# File_name = dt_string + 'Table_Series_Netflix.csv'

# salvando o .csv na pasta local
(
    df_csv
      .coalesce(1)
      .write
      .options(delimiter=';', header='true', inferSchema='true')
      .mode('overwrite')
      .csv(Bucket_destination +'/' + folder_destination_sytem)
)

# vários outros arquivos são gerados além do csv e para facilitar na hora de renomear e fazer upload, eles serão removidos da pasta local
for f in os.listdir(Bucket_destination +'/' + folder_destination_sytem):
  if f[-3:] != 'csv':
    os.remove(os.path.join(Bucket_destination +'/' + folder_destination_sytem, f))

# renomeando o csv que será armazenado no S3
csv_file = os.listdir(Bucket_destination +'/' + folder_destination_sytem)[-1]
source = Bucket_destination +'/' + folder_destination_sytem +'/' + csv_file
dest = path_FileCsv_system
os.rename(source, dest)

"""## Nesta próxima célula, finalmente salvamos o .csv no S3"""

file_name_source = Bucket_destination +'/' + folder_destination_sytem +'/' + dt_string + 'Table_Series_Netflix.csv' #origem local
File_name_dest = 'refined_data_Netflix/' + dt_string + 'Table_Series_Netflix.csv' # destino s3
all_obj_bucket = list(s3.Bucket('detch-bucket01').objects.all()) # já havíamos instanciado, mas é necessário atualizar...

try:
    if s3.ObjectSummary(Bucket_destination, File_name_dest) not in all_obj_bucket:
      print('Arquivo não existente...  criando\n')
    else: 
      print('Arquivo já existe e será sobrescrito\n')
    # fazendo nessa próxima linha o upload
    s3_client.upload_file(file_name_source, Bucket_destination, File_name_dest)
    # mostrando onde o arquivo foi salvo
    path = s3.ObjectSummary(Bucket_destination, directory_destination).Bucket().name + '/' + s3.ObjectSummary(Bucket_destination, File_name_dest).key
    print('Upload de arquivo realizado com sucesso\n')
    print('O caminho do mesmo é: {}'.format(path))
    print('\nRemovendo o Arquivo do FileSystem\n')
    os.remove(file_name_source)# excluindo o arquivo do armazenamento local após upload na nuvem
except ClientError as e:
    logging.error(e)